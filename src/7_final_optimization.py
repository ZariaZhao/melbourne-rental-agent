import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib  # ç”¨äºä¿å­˜æ¨¡å‹
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder


# 1. å‡†å¤‡æ•°æ®
df = pd.read_csv('melb_data.csv')

# ç‰¹å¾å·¥ç¨‹ (ä¿æŒä¸å˜)
df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)
df['Year'] = df['Date'].dt.year
df['YearBuilt'] = pd.to_numeric(df['YearBuilt'], errors='coerce').fillna(df['YearBuilt'].median())
df['House_Age'] = df['Year'] - df['YearBuilt']
df['Landsize'] = df['Landsize'].fillna(df['Landsize'].median())
df['BuildingArea'] = df['BuildingArea'].fillna(df['BuildingArea'].median())

# ==========================================
# âœ‚ï¸ ç˜¦èº«æ—¶åˆ» (Feature Selection)
# ==========================================
# ä¹‹å‰çš„å…¨é‡ç‰¹å¾ï¼š
# features_full = ['Rooms', 'Type', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Year', 'House_Age', 'Lattitude', 'Longtitude']

# âœ… ç²¾ç®€åçš„â€œç‰¹ç§éƒ¨é˜Ÿâ€ (åªç•™ Top 8)ï¼š
features_slim = [
    'Lattitude', 'Longtitude',  # æ ¸å¿ƒåœ°æ®µ
    'Rooms',                    # æ ¸å¿ƒå¤§å°
    'Distance',                 # æ ¸å¿ƒä½ç½®
    'Landsize', 'BuildingArea', # æ ¸å¿ƒèµ„äº§ä»·å€¼
    'Type',                     # æ ¸å¿ƒæˆ¿å‹ (Pipeline ä¼šè‡ªåŠ¨è½¬ One-Hotï¼Œç”Ÿæˆ Type_u ç­‰)
    'Bathroom',                 # æ ¸å¿ƒé…ç½® (å¯ŒäººåŒºé€šå¸¸å•æ‰€å¤š)
    'House_Age'                 # æ ¸å¿ƒæŠ˜æ—§
]

# âŒ åˆ æ‰äº†ï¼šBedroom2, Car, Year (æ³¨æ„ï¼šType ä¼šè‡ªåŠ¨å¤„ç†ï¼Œä¸ç”¨æ‰‹åŠ¨åˆ  Type_h)

X = df[features_slim]
y = df['Price']

# 2. Pipeline (ä¿æŒä¸å˜)
numeric_cols = [c for c in X.columns if c != 'Type']
categorical_cols = ['Type']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='median'), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', RandomForestRegressor(n_estimators=100, random_state=1))
])

# 3. éªŒè¯ç˜¦èº«ç»“æœ
cv = KFold(n_splits=5, shuffle=True, random_state=1)
print(f"â³ Testing Slim Model with {len(features_slim)} features...")

scores = cross_val_score(pipeline, X, y, cv=cv, scoring='r2')

print("\n" + "="*40)
print("ğŸš€ FINAL SLIM MODEL RESULTS")
# ... (ä¸Šé¢çš„ä»£ç ä¿æŒä¸å˜)

print("\n" + "="*40)
print("ğŸš€ FINAL SLIM MODEL RESULTS")
print("="*40)
print(f"Previous Full Model RÂ²: 0.8044")
print(f"Current Slim Model RÂ² : {scores.mean():.4f} (Â± {scores.std():.4f})")
print("="*40)

# ==========================================
# ğŸ¨ 1. ç”»å›¾ï¼šé¢„æµ‹å€¼ vs çœŸå®å€¼ (Fixing NameError)
# ==========================================
print("ğŸ¨ Generating Prediction vs Actual plot...")

# å…³é”®ä¿®å¤ç‚¹ï¼šè¿™é‡Œè®¡ç®—äº† y_predï¼Œä½ çš„æŠ¥é”™å°±æ˜¯å› ä¸ºç¼ºäº†è¿™ä¸€è¡Œï¼
y_pred = cross_val_predict(pipeline, X, y, cv=cv)

plt.figure(figsize=(8, 8))
plt.scatter(y, y_pred, alpha=0.3, color='blue')
# ç”»ä¸€æ¡çº¢è‰²çš„å®Œç¾å¯¹è§’çº¿
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
plt.xlabel('Actual Price (çœŸå®ä»·æ ¼)')
plt.ylabel('Predicted Price (é¢„æµ‹ä»·æ ¼)')
plt.title('Truth vs. Prediction')
# ä¿å­˜å›¾ç‰‡
plt.savefig('prediction_scatter.png')
print("âœ… Plot saved as 'prediction_scatter.png'")
# plt.show() # å¦‚æœä¸æƒ³å¼¹çª—ï¼Œå°±ä¿æŒæ³¨é‡ŠçŠ¶æ€

# ==========================================
# ğŸ“¦ 2. ä¿å­˜æ¨¡å‹ (Saving Model)
# ==========================================
if scores.mean() >= 0.795:
    print("\nğŸ“¦ Performance is good. Retraining on 100% data...")
    
    # ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
    pipeline.fit(X, y)
    
    # ä¿å­˜æ–‡ä»¶
    model_filename = 'melbourne_housing_model.pkl'
    joblib.dump(pipeline, model_filename)
    
    print(f"âœ… Model saved successfully as: {model_filename}")
else:
    print("âŒ Performance not good enough. Model not saved.")